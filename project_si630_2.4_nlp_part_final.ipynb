{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c758056a-49cd-487e-86e5-f63e0ede3058",
   "metadata": {},
   "source": [
    "## The NLP part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68784d1-8d17-47a5-810c-0f21a4a5394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f8d591-66a0-492d-80e7-926445969394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "rs = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcecf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70f8974f-7912-4e06-802f-80090f51f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        self.labels = []\n",
    "        self.output = []\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "        \n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "\n",
    "        # Step 1: Read the file\n",
    "        print('Reading data and tokenizing')\n",
    "\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "#         raw_data = raw_data[raw_data['lyrics'].notna()]\n",
    "        raw_x = raw_data['lyrics'].to_list()\n",
    "        raw_labels = raw_data['data.album.tracks.items.track.playcount'].to_list()\n",
    "        self.labels = np.log(raw_labels)\n",
    "        \n",
    "        all_tokens = []\n",
    "        cachedStopWords = stopwords.words('english')\n",
    "        \n",
    "        for track in raw_x:\n",
    "            \n",
    "            track_re = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", track)\n",
    "            token = self.tokenize(track_re)\n",
    "            token_without_sw = [word for word in token if not word in cachedStopWords]\n",
    "            token_without_sw_l = [word.lower() for word in token_without_sw]\n",
    "            all_tokens.append(token_without_sw_l)\n",
    "            \n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        freqs = Counter(list(itertools.chain.from_iterable(all_tokens)))\n",
    "        \n",
    "        # Step 3: Remove low occurrence\n",
    "#         Replace all tokens below the specified frequency with an <UNK> and then remove them\n",
    "\n",
    "        print(\"Performing minimum thresholding\")\n",
    "\n",
    "        for track in all_tokens:\n",
    "            for index in range(len(track)):\n",
    "                if freqs[track[index]] < min_token_freq:\n",
    "                    track[index] = '<UNK>'\n",
    "\n",
    "                \n",
    "        for track in all_tokens:\n",
    "            t = []\n",
    "            for word in track:\n",
    "                if word != '<UNK>':\n",
    "                    t.append(word)\n",
    "                \n",
    "            self.output.append(t)\n",
    "    \n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "\n",
    "        self.word_counts = Counter(list(itertools.chain.from_iterable(self.output)))\n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping. \n",
    "\n",
    "        n = 0\n",
    "        for i in self.word_counts.keys():\n",
    "            self.word_to_index[i] = n\n",
    "            self.index_to_word[n] = i\n",
    "\n",
    "            n += 1\n",
    "        \n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d usable tracks (%d unique words)' \\\n",
    "              % (file_name, len(corpus.output),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def get_prediction_tensor(self, lyric):\n",
    "        cachedStopWords = stopwords.words('english')\n",
    "        # cachedStopWords = []\n",
    "        lyric_re = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", lyric)\n",
    "        t = self.tokenize(lyric_re)\n",
    "        t_without_sw = [word for word in t if not word in cachedStopWords]\n",
    "        t_without_sw_l = [word.lower() for word in t_without_sw]\n",
    "        \n",
    "        tra_dict = Counter(t_without_sw_l)\n",
    "        \n",
    "        r = []\n",
    "        c = []\n",
    "        d = []\n",
    "        for word in tra_dict.keys():\n",
    "            try:\n",
    "                c.append(self.word_to_index[word])\n",
    "                d.append(float(tra_dict[word]))\n",
    "                r.append(0)\n",
    "            except:\n",
    "                continue\n",
    "        i = [r, c]\n",
    "        s = torch.sparse_coo_tensor(i, d, (1, len(corpus.word_counts)))\n",
    "        return s.to_dense()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f128dc5a-a280-4047-80d1-8e4aaeafc509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from 20220413_all.csv; saw 3343 usable tracks (7212 unique words)\n"
     ]
    }
   ],
   "source": [
    "csv_name = '20220413_all.csv'\n",
    "corpus = Corpus()\n",
    "# corpus.load_data('20220311_training.csv', 5)\n",
    "# corpus.load_data('train_0412_oldest.csv', 5)\n",
    "corpus.load_data(csv_name, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5a302a8-e67f-4ee7-8743-a1b7a7c324bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build input data\n",
    "tracks = corpus.output\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "\n",
    "row_num = 0\n",
    "for track in tracks:\n",
    "    track_dict = Counter(track)\n",
    "    for word in track_dict.keys():\n",
    "        col.append(corpus.word_to_index[word])\n",
    "        # frequency\n",
    "        # data.append(float(track_dict[word]))\n",
    "        # TD-IDF\n",
    "        data.append(float(track_dict[word])/sum(track_dict.values()))\n",
    "        row.append(row_num)\n",
    "    row_num += 1\n",
    "    \n",
    "i = [row, col]\n",
    "\n",
    "s_input = torch.sparse_coo_tensor(i, data, (len(tracks), len(corpus.word_counts))).to_dense()\n",
    "labels = torch.FloatTensor(corpus.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97e8239d-6fa8-4d57-a872-92bc0b4c370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Counter(tracks[0]).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d7a2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine numerical\n",
    "raw_data = pd.read_csv(csv_name)\n",
    "\n",
    "numeric = raw_data[['danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
    "          'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']].to_numpy()\n",
    "\n",
    "t = torch.from_numpy(numeric)\n",
    "num_only_input = t.float()\n",
    "s_input_new = torch.cat((s_input, t), 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89a6d86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_input.size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57d3ec4a-5637-40cc-8939-b762a191c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(s_input, # s_input_new for num+text, s_input for text only, num_only_input for numeric only\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39e88a0d-33b3-4687-a79a-183be1fa6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.numpy()\n",
    "y_train = y_train.numpy()\n",
    "X_test = X_test.numpy()\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d8bb7-1cf8-4c83-a8d4-1db41bd6bdc9",
   "metadata": {},
   "source": [
    "### Baseline 1: Linear Regression (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5619150-8b83-44e6-9663-fb349a6b4942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.16285 , 16.409067, 17.97158 , ..., 19.799917, 18.86811 ,\n",
       "       18.08887 ], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6edec35d-a66a-4eff-aacf-7e02826f8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae008031-98ea-41f6-954e-e092c1fc4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_lr =  LinearRegression(positive=True)\n",
    "model_lr =  LinearRegression(positive=False) # for numeric only\n",
    "                                    # criterion='squared_error')\n",
    "\n",
    "# para_lr = {\n",
    "#             'n_estimators': [200, 250, 300, 350, 400],\n",
    "#             'max_depth': [10, 20, 30, 40, 50]\n",
    "#             }\n",
    "# g_lr = GridSearchCV(model_lr, para_lr, cv=5, n_jobs=-1, verbose=1)\n",
    "# g_lr.fit(X_train, y_train)\n",
    "# model_lr_f = g_lr.best_estimator_\n",
    "model_lr.fit(X_train, y_train)\n",
    "model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e2943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "def evaluation_sk(X_test, y_test, model):\n",
    "#     test_data = pd.read_csv(test_file)\n",
    "#     labels = np.log(test_data['data.album.tracks.items.track.playcount'].to_list())\n",
    "#     lyrics = test_data['lyrics'].to_list()\n",
    "    \n",
    "#     pred_ys = []\n",
    "#     for lyric in lyrics:\n",
    "#         pred_y = model.predict(corpus.get_prediction_tensor(lyric).numpy())[0].tolist()\n",
    "#         pred_ys.append(pred_y)\n",
    "    pred_y = model.predict(X_test)\n",
    "    \n",
    "    return [mean_absolute_error(y_test, pred_y), mean_squared_error(y_test, pred_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecb52619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1288.4697, 3855371.5]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_sk(X_test, y_test, model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85043871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b09b8-a742-4441-880a-992a0395a58b",
   "metadata": {},
   "source": [
    "### Baseline 2: random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35614800-1c5b-4293-908d-391d67f773ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6dab0f1-a351-477d-a366-0582731480da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_rf =  RandomForestRegressor(\n",
    "#                                     random_state=rs, \n",
    "# #                                     criterion='squared_error',\n",
    "#                         #             max_depth=None, \n",
    "#                                     min_samples_split=2)\n",
    "\n",
    "# para_rf = {\n",
    "#             'n_estimators': [200, 250, 300, 350, 400],\n",
    "#             'max_depth': [10, 20, 30, 40, 50]\n",
    "#             }\n",
    "# g_rf = GridSearchCV(model_rf, para_rf, cv=5, n_jobs=-1, verbose=1)\n",
    "# g_rf.fit(X_train, y_train)\n",
    "# model_rf_f = g_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40357011-595a-4cf7-b420-4f1bf176a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_rf_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02b0acb5-c739-44ea-b887-fd18cbdf487d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=30, n_estimators=350, random_state=42)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf_f = RandomForestRegressor(\n",
    "                                    random_state=rs, \n",
    "                                    min_samples_split=2,\n",
    "                                    n_estimators = 350,\n",
    "                                    max_depth = 30\n",
    ")\n",
    "model_rf_f.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c1c55dd-3be1-4512-8862-fddf047bb2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2516953080814168, 2.647404537225213]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_sk(X_test, y_test, model_rf_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a68a56-3037-48da-a42c-d5075bbbb771",
   "metadata": {},
   "source": [
    "### Baseline 3: random dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccd88d14-07df-4ba0-8bef-f6b1e90b152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8c9489a-9393-4e66-bfb7-2baeb3de3e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyRegressor()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dummy = DummyRegressor(strategy=\"mean\")\n",
    "model_dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7d80802-ae82-492b-a5b5-4958ecdba027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5806782, 3.8248508]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_sk(X_test, y_test, model_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557171a-b234-4728-a4c5-1eb1649e8eb4",
   "metadata": {},
   "source": [
    "### Model 1: SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8e397b3-4fd9-474f-8c80-63e4c40111ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae259f55-8969-44e7-b0e7-d8671300bb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    }
   ],
   "source": [
    "# model_svm = SVR(kernel='rbf')\n",
    "\n",
    "# para_svm = {\n",
    "#             'C' : [0.1, 0.5, 1, 1.5, 2],\n",
    "#             'epsilon': [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "#             }\n",
    "# g_svm = GridSearchCV(model_svm, para_svm, cv=5, n_jobs=-1, verbose=1)\n",
    "# g_svm.fit(X_train, y_train)\n",
    "# model_svm_f = g_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47c05037-b8be-4efa-b4eb-2b683d9c7c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1, epsilon=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm_f = SVR(kernel='rbf', C=1, epsilon=0.01)\n",
    "model_svm_f.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e6f7a48-7900-4e77-aa65-9acdae82bd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.318409096253149, 2.9159290886651443]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_sk(X_test, y_test, model_svm_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f6f38-8ca4-486c-bd0c-42c8657d7bb2",
   "metadata": {},
   "source": [
    "### Model 2: BERT miniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04dceb9-3bb9-4dd1-9f53-ce5d49fd3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95337b4c-509e-4576-a9d3-17d66ef27134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26337de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62f6426-8337-4b56-b0de-a9638ef42437",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d355581-5702-4242-97f5-b22933048fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('train_0412_oldest.csv')\n",
    "# eval_data = pd.read_csv('20220310_testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198f5ae9-7a14-4cbc-8b5a-df4d8960a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('20220413_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dada6b71-a377-476a-88a3-a365fbed63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_dataset(df, load_lyrics = True, load_numeric = False, is_pred = False):\n",
    "       \n",
    "    if load_numeric == True:\n",
    "        def n2t(danceability, energy, key, loudness, speechiness,\n",
    "                acousticness, instrumentalness, liveness, valence, tempo):\n",
    "            out = f\"The danceability is {str(danceability)}. [SEP] The energy is {str(energy)}. [SEP] The key is {str(key)}. [SEP] The loudness is {str(loudness)}. [SEP] The speechiness is {str(speechiness)}. [SEP] The acousticness is {str(acousticness)}. [SEP] The instrumentalness is {str(instrumentalness)}. [SEP] The liveness is {str(liveness)}. [SEP] The valence is {str(valence)} [SEP] The tempo is {str(tempo)}.\"\n",
    "            return out\n",
    "\n",
    "        df['num2text'] = df.apply(lambda x: n2t(x.danceability, \n",
    "                                                    x.energy, \n",
    "                                                    x.key, \n",
    "                                                    x.loudness, \n",
    "                                                    x.speechiness, \n",
    "                                                    x.acousticness, \n",
    "                                                    x.instrumentalness, \n",
    "                                                    x.liveness, \n",
    "                                                    x.valence, \n",
    "                                                    x.tempo), axis=1)\n",
    "        \n",
    "        if load_lyrics == True:\n",
    "            df['text'] = df[['num2text', 'lyrics']].agg(' [SEP] '.join, axis=1)\n",
    "        else:\n",
    "            df['text'] = df['num2text']\n",
    "    else:\n",
    "        if load_lyrics == True:\n",
    "            df['text'] = df['lyrics']\n",
    "        else:\n",
    "            print(\"At least one of load_lyrics or load_numeric must be true.\")\n",
    "            exit()\n",
    "    \n",
    "    if is_pred == True:\n",
    "        data = df['text']\n",
    "        data['text'] = df['text'].str.replace(r'[\\(\\[].*?[\\)\\]]', '')\n",
    "#         data = data.rename(columns={'lyrics': 'text'})\n",
    "        output = data\n",
    "        \n",
    "    else:\n",
    "        data = df[['text','data.album.tracks.items.track.playcount']]\n",
    "        data['text'] = df['text'].str.replace(r'[\\(\\[].*?[\\)\\]]', '')\n",
    "#         data = data.rename(columns={'lyrics': 'text'})\n",
    "        data['labels'] = np.log(data['data.album.tracks.items.track.playcount'])\n",
    "    \n",
    "        output = Dataset.from_pandas(data, preserve_index=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4689e808-a94a-46f0-955d-f6da7ea17b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a83126-52b8-43eb-a392-20ed4a31ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = load_data_to_dataset(train_data, is_pred = False)\n",
    "# train_ds = train_ds.map(tokenize_text, batched=True)\n",
    "\n",
    "# eval_ds = load_data_to_dataset(eval_data, is_pred = False)\n",
    "# eval_ds = eval_ds.map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fffd991a-fb6e-4ce3-a5c5-234c6221ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-f4ae310a6bc2>:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = df['text'].str.replace(r'[\\(\\[].*?[\\)\\]]', '')\n",
      "<ipython-input-9-f4ae310a6bc2>:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = df['text'].str.replace(r'[\\(\\[].*?[\\)\\]]', '')\n",
      "<ipython-input-9-f4ae310a6bc2>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['labels'] = np.log(data['data.album.tracks.items.track.playcount'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf88a4ab1ae43c1bf77e46f204e0908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_data_to_dataset(raw_data, load_lyrics = True, load_numeric = True, is_pred = False)\n",
    "dataset = dataset.map(tokenize_text, batched=True)\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = dataset_dict['train']\n",
    "eval_ds = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "884297c3-2207-484c-b25e-94aa4d459fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The danceability is 0.921.  The energy is 0.472.  The key is 10.  The loudness is -7.88.  The speechiness is 0.43.  The acousticness is 0.164.  The instrumentalness is 0.432.  The liveness is 0.156.  The valence is 0.198  The tempo is 139.997.   Lot of shots  Lot of shots  30s on Glocks  30 shots  Lot of shots  Lot of shots  30s on Glocks  Lot of shots  30s on Glocks  Put you on Fox  Nigga we ain't goin'  Lot of shots  30s on Glocks  30s on Glocks If Young Metro don't trust you, I'm gon' shoot you  Lot of shots, hold up 30s on Glocks, hold up Put you on Fox, hold up Put 'em in a casket, yeah Pull up on a Banshee, yeah Nigga, fuck your handshake, yeah Feel like I'm the last real rapper 'cause these niggas weird Nah, these niggas queers Sippin' Act, Cheers In my own lane, nigga, watch where you steer Think I got 'em scared, shot 'em in the beard That's a chin check, I'm certified everywhere Man, I'm certified for real, nigga Nah, for real, nigga Used to record right on deal, nigga Nah, for real, nigga Pillow talking get you killed, nigga Nah, for real, nigga Bronx niggas say I'm ill nigga Nah, for real, nigga Bought a pawn shop for real, bitch Nah, for real, bitch Keep my gun cocked for real, bitch Nah, for real, bitch Nigga, we bond drop for real, bitch Nah, for real, bitch Cut your lawns off for real, bitch Nah, for real, bitch\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test']['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b103b054-d995-4187-82d6-16db95ae9bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/conanwu/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/MiniLM-L12-H384-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/pytorch_model.bin from cache at /home/conanwu/.cache/huggingface/transformers/b774244369e464de2c660477b70bae7c3223fa7250aa1c8fc0b0f037ed58418a.087808d17814e241e9352c5ce0fea1a7d05e5b0f020d44b42b5f05922e96c923\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification as AMFSC\n",
    "model = AMFSC.from_pretrained(model_ckpt, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "288087ec-48f7-4855-86ab-260eb7e83bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    return {'mae': mae, 'mse':mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e55dd647-59c9-47ee-8702-32e2f3161c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "batch_size = 32\n",
    "\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "output_dir = 'minln-finetuned-regression'\n",
    "training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=5e-4,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False,\n",
    "                                  fp16=True\n",
    "                                 )\n",
    "\n",
    "trainer = Trainer(model = model, \n",
    "                  args = training_args, \n",
    "                  train_dataset = train_ds, \n",
    "                  eval_dataset = eval_ds,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5da16bf3-54da-402d-b963-193cb4ee00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, data.album.tracks.items.track.playcount. If text, data.album.tracks.items.track.playcount are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/conanwu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3008\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 470\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 02:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>68.955500</td>\n",
       "      <td>3.635123</td>\n",
       "      <td>1.536411</td>\n",
       "      <td>3.635123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.002000</td>\n",
       "      <td>3.557034</td>\n",
       "      <td>1.516624</td>\n",
       "      <td>3.557034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.029400</td>\n",
       "      <td>3.579984</td>\n",
       "      <td>1.522502</td>\n",
       "      <td>3.579984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.093300</td>\n",
       "      <td>3.590840</td>\n",
       "      <td>1.525097</td>\n",
       "      <td>3.590839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.033500</td>\n",
       "      <td>3.567364</td>\n",
       "      <td>1.519533</td>\n",
       "      <td>3.567364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, data.album.tracks.items.track.playcount. If text, data.album.tracks.items.track.playcount are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 335\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, data.album.tracks.items.track.playcount. If text, data.album.tracks.items.track.playcount are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 335\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, data.album.tracks.items.track.playcount. If text, data.album.tracks.items.track.playcount are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 335\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, data.album.tracks.items.track.playcount. If text, data.album.tracks.items.track.playcount are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 335\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, data.album.tracks.items.track.playcount. If text, data.album.tracks.items.track.playcount are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 335\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=470, training_loss=17.02275910073138, metrics={'train_runtime': 131.5059, 'train_samples_per_second': 114.367, 'train_steps_per_second': 3.574, 'total_flos': 990708072775680.0, 'train_loss': 17.02275910073138, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299cc4a-c76e-477e-bddb-5074a8f8fb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
